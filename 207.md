+++
title = "MATH 207 | Notes"
hascode = true
rss = "notes for MATH-207"
rss_title = "MATH 207 | Notes"
rss_pubdate = Date(2024, 3, 19)

tags = ["differential equations", "linear algebra"]
+++

# Differential Equations and Linear Algebra

\tableofcontents

## Vector Spaces

**Definition**: A **real vector space** is a set $V$ with the following properties:

1) $\forall\vector{u}, \vector{v}\in V, \vector{u} + \vector{v}\in V$
2) $\forall\vector{u}\in V, \forall c\in\R, c\vector{u}\in V$
3) $\forall\vector{u}, \vector{v}\in V, \vector{u} + \vector{v} = \vector{v} + \vector{u}$
4) $\forall\vector{u}, \vector{v}, \vector{w}\in V, (\vector{u} + \vector{v}) + \vector{w} = \vector{u} + (\vector{v} + \vector{w})$
5) $\forall\vector{u}\in V, \exists\vector{0}\in V, \vector{u} + \vector{0} = \vector{0} + \vector{u} = \vector{u}$
6) $\forall\vector{u}\in V, \exists\vector{-u}\in V, \vector{u} + (-\vector{u}) = \vector{0}$
7) $\forall\vector{u}\in V, 1\vector{u} = \vector{u}$
8) $\forall\vector{u}\in V, \forall a, b\in\R, a(b\vector{u}) = (ab)\vector{u}$
9) $\forall\vector{u}, \vector{v}\in V, \forall c\in\R, c(\vector{u}+\vector{v}) = c\vector{u} + c\vector{v}$
10) $\forall\vector{u}\in V, \forall a, b\in\R, (a+b)\vector{u} = a\vector{u} + b\vector{u}$.

**Theorem**: If $V$ is a real vector space, then

1) the zero vector is unique
2) $\forall\vector{u}\in V, 0\vector{u} = \vector{0}$
3) $\forall c\in\R, c\vector{0} = \vector{0}$
4) additive inverses are unique
5) $\forall\vector{u}\in V, -\vector{u} =(-1)\vector{u}$
6) $\forall\vector{u}\in V, \forall c\in\R$ if $c\vector{u} = \vector{0}$, then $\vector{u} = 0$, $c = 0$, or both.

**Example**: The set of all $1 \times n$ row vectors is a vector space.

**Example**: The set of all $m \times 1$ column vectors is a vector space.

**Example**: The set of all $2 \times 2$ matrices with real entries is a vector space.

**Example**: Let $I \subseteq\R$ be an interval. The set of all real-valued functions on $I$ is a vector space.

**Example**: The set of all solutions of the differential equation $y'' + y' + y = 0$ is a vector space.

### Subspaces

**Definition**: Let $V$ be a real vector space. Let $W$ be a subset of $V$. Then $W$ is a **subspace** of $V$ if $W$ itself is a vector space using the same vector addition and scalar multiplication operations of $V$.

For the following examples, suppose $V$ is a vector space.

**Example**: If $W\subseteq V$ and $W = \{\vector{0}\}$, then $W$ is called the **trivial subspace**.

**Example**: If $W\subseteq V$ and $W = V$, then $W$ is called the **whole subspace** (also known as the whole space).

**Non-Example**: If $W\subseteq V$ and $W = \varnothing$, $W$ fails to have a zero vector, therefore it is not a subspace of $V$.

**Theorem**: If $V$ is a vector space and $W\subseteq V$, to check if $W$ is a subspace of $V$ you need only verify three things:

1) $\forall\vector{u}, \vector{v}\in W, \vector{u} + \vector{v}\in W$
2) $\forall\vector{u}\in W, \forall c\in\R, c\vector{u}\in W$
3) $\exists\vector{0}\in W$.

### Spanning Sets

#### Linear Combinations

**Definition**: A **linear combination** of a set of vectors $\vector{u_{1}}, \vector{u_{2}}, \vector{u_{3}}, ..., \vector{u_{n}}$ is of the form
\begin{equation*}
a_{1}\vector{u_{1}} + a_{2}\vector{u_{2}} + a_{3}\vector{u_{3}} + ... + a_{n}\vector{u_{n}}
\end{equation*}
where $a_{1}, a_{2}, a_{3}, ..., a_{n}\in\R$.

**Definition**: The set of all such linear combinations is called the **span** of $\{\vector{u_{1}}, ..., \vector{u_{n}}\}$ and is denoted by
\begin{equation*}
\text{span}\{\vector{u_{1}}, ..., \vector{u_{n}}\} = \{a_{1}\vector{u_{1}} + a_{2}\vector{u_{2}} + a_{3}\vector{u_{3}} + ... + a_{n}\vector{u_{n}}:a_{1}, ..., a_{n}\in\R\}.
\end{equation*}

**Theorem**: The span of a set of vectors in $V$ is a subspace of $V$.

**Definition**: If $V = \text{span}\{\vector{u_{1}}, ..., \vector{u_{n}}\}$, $\{\vector{u_{1}}, ..., \vector{u_{n}}\}$ is a spanning set for $V$.

### Linear Independence

Consider the equation $a_{1}\vector{u_{1}} + a_{2}\vector{u_{2}} + a_{3}\vector{u_{3}} + ... + a_{n}\vector{u_{n}} = \vector{0}$.

**Definition**: A set of vectors $\{\vector{u_{1}}, ..., \vector{u_{n}}\}$ is called **linearly independent** if the only solution to the equation above is $a_{1} = 0, a_{2} = 0, a_{3} = 0, ..., a_{n} = 0$ which is also known as the **trivial solution**.

**Definition**: A set of vectors $\{\vector{u_{1}}, ..., \vector{u_{n}}\}$ is called **linearly dependent** if there exists a solution to the equation above where $a_{1}, a_{2}, a_{3}, ..., a_{n}$ aren't all zero.

**Theorem**: Suppose $\{\vector{u_{1}}, ..., \vector{u_{n}}\}$ is a linearly dependent set of vectors and $\vector{u_{n}}$ can be written as a linear combination of $\{\vector{u_{1}}, ..., \vector{u_{n-1}}\}$. Then,
\begin{equation*}
\text{span}\{\vector{u_{1}}, ..., \vector{u_{n}}\} = \text{span}\{\vector{u_{1}}, ..., \vector{u_{n-1}}\}.
\end{equation*}

The equation $a_{1}\vector{u_{1}} + a_{2}\vector{u_{2}} + a_{3}\vector{u_{3}} + ... + a_{n}\vector{u_{n}} = \vector{0}$ is equivalent to the system of linear homogeneous equations $A\vector{a} = \vector{0}$ where $A$ is the $m \times n$ matrix whose column vectors are $\vector{u_{1}}, ..., \vector{u_{n}}$ and $\vector{a}$ is the vector whose entries are $a_{1}, ..., a_{n}$.

**Theorem**: Let $A$ be the $m \times n$ matrix whose column vectors are $\vector{u_{1}}, ..., \vector{u_{n}}$. Then, $\{\vector{u_{1}}, ..., \vector{u_{n}}\}$ is linearly independent if and only if the only solution to $A\vector{a} = \vector{0}$ is the trivial solution $\vector{a} = \vector{0}$. Equivalently, this is true if and only if the system has no free variables.

#### The Wrońskian

**Definition**: Let $f_{1}(x), ..., f_{n}(x)$ be an ordered set of $n$ functions which are all differentiable at least $n-1$ times. The **Wrońskian** of these functions is the determinant expression
\begin{equation*}
W(f_{1}(x), ..., f_{n}(x)) = \text{det}\left(\begin{array}{cccc}f_{1}(x) & f_{2}(x) & \dots & f_{n}(x)\\ f_{1}'(x) & f_{2}'(x) & \dots & f_{n}'(x)\\ \vdots & \vdots & \ddots & \vdots\\ f_{1}^{(n-1)}(x) & f_{2}^{(n-1)}(x) & \dots & f_{n}^{(n-1)}(x)\end{array}\right).
\end{equation*}

**Theorem**: Suppose that the Wrońskian $W(f_{1}(x), ..., f_{n}(x))$ is nonzero on at least one value in an interval $I$. Then, the functions $f_{1}(x), ..., f_{n}(x)$ are linearly independent over the interval $I$.

Note that in the theorem above, this is not an if and only if statement. It is possible for the Wrońskian to be zero and the ordered set of functions to be linearly independent.

### Bases of a Vector Space

**Definition**: A **basis** for a vector space is a set of vectors that both span the space and are linearly independent.

**Definition**: A vector space is called **finite dimensional** if it has a finite basis and **infinite dimensional** if every basis is infinite.

**Theorem**: If $S$ is a spanning set of $V$ with $n$ elements and $L$ is a linearly independent subset of $V$, then $L$ has at most $n$ elements.

**Corollary**: Suppose that $B$ is basis for $V$ consisting of $n$ elements. Then, any set with less than $n$ elements cannot span the space and any set with more than $n$ elements cannot be linearly independent.

#### Dimension of a Vector Space

**Definition**: If $V$ is a finite dimensional vector space, we call the cardinality of its bases the **dimension** of $V$ denoted as $\text{dim}(V)$.

**Theorem**: If $B_{1}$ and $B_{2}$ are each a basis for the same vector space $V$ and one of them is finite, then both bases are finite and have the same dimension.

**Theorem**: Suppose $V$ is $n$-dimensional. If $S$ is a finite spanning set of vectors in $V$, then $S$ contains a basis for $V$ and $S$ has at least $n$ elements.

**Corollary**: If $\text{dim}(V) = n$, then any set of $n$ spanning vectors is basis.

**Theorem**: Suppose $V$ is $n$-dimensional. If $S$ is a finite set of linearly independent vectors in $V$, then $S$ has at most $n$ elements which can be extended to become a basis.

**Corollary**: If $\text{dim}(V) = n$, then any set of $n$ linearly independent vectors is basis.

## Linear Transformations

Consider $T: V \rightarrow W$.

$T$ is a **map** where $V$ is the **domain** and $W$ is the **codomain**. Linear transformations (also known as linear mappings) are special types of maps.

**Definition**: A **linear transformation** is a function $T: V \rightarrow W$ with the properties that $T$:

- preserves scalar multiplication, $T(c\vector{v}) = cT(\vector{v})$
- preserves vector addition, $T(\vector{u}+\vector{v}) = T(\vector{u}) + T(\vector{v})$.

**Example**: $D: C^{n}(I) \rightarrow C^{n-1}(I)$, $D(f(x))=f'(x)$

Note: $I$ is an interval and $C^{n}(I)$ is a set of functions on $I$ with at least $n$ derivatives and $f^{n}(x)$ is continuous.

To prove $D$ is a linear transformation, we need to show $D$ satisfies three things:

- it is well defined
- preserves scalar multiplication
- preserves vector addition.

Check 1) Well-defined

Take $f(x)\in C^{n}(I)$. Then, $f'(x), f''(x), f'''(x), ..., f^{n}(x)$ all exist and are continuous on $I$. Therefore, $f'(x)$ exists and $f'(x)\in C^{n-1}(I)$ is equal to $f'(x)\in C^{n-1}(I)$ and is well defined.

Check 2) Scalar multiplication

Take $f(x)\in C^{n}(I)$ and $c\in\R$. Then,
\begin{align*}
D(cf(x)) &= (cf(x))'\\
         &= cf'(x)\\
         &= cD(f(x))
\end{align*}
so, $D(cf(x)) = cD(f(x))$.

Check 3) Vector addition

Take $f(x), g(x)\in C^{n}(I)$. Then,
\begin{align*}
D(f(x)+g(x)) &= (f(x)+g(x))'\\
             &= f'(x) + g'(x)\\
             &= D(f(x)) + D(g(x))
\end{align*}
so, $D(f(x)+g(x)) = D(f(x)) + D(g(x))$.

Therefore, $D$ is a linear transformation.

**Example**: $T: C^{2}(I) \rightarrow C^{0}(I), T(f(x)) = a_{2}(x)f''(x)+a_{1}f'(x)+a_{0}(x)f(x)$ where $a_{2}(x), a_{1}(x), a_{0}(x)\in C^{0}(I)$

Check 1) Well defined

$f(x), f'(x), f''(x)$ are continuous so $a_{2}(x)f''(x) + a_{1}f'(x) + a_{0}(x)f(x)$ is continuous, thus $T$ is well defined.

Check 2) Scalar multiplication

Take $f(x)\in C^{2}(I), c\in\R$.

\begin{align*}
T(cf(x)) &= a_2(x)f''(cx) + a_1f'(cx) + a_0(x)f(cx)\\
         &= ca_2(x)f''(x) + ca_1f'(x) + ca_0(x)f(x)\\
         &= c(a_2(x)f''(x) + a_1f'(x) + a_0(x)f(x))\\
         &= cT(f(x))
\end{align*}

Check 3) Vector addition

Take $f(x), g(x)\in C^{2}(I)$.

\begin{align*}
T(f(x)+g(x)) &= a_2(x)(f(x)+g(x))'' + a_1(f(x)+g(x))' + a_0(x)(f(x)+g(x))\\
             &= a_2(x)(f''(x)+g''(x)) + a_1(f'(x)+g'(x)) + a_0(x)(f(x)+g(x))\\
             &= a_2(x)f''(x) + a_2(x)g''(x) + a_1(x)f'(x) + a_1(x)g'(x) + a_0(x)f(x) + a_0(x)g(x)\\
             &= a_2(x)f''(x) + a_1(x)f'(x) + a_0(x)f(x) + a_2(x)g''(x) + a_1(x)g'(x) + a_0(x)g(x)\\
             &= T(f(x)) + T(g(x))
\end{align*}

Therefore, $T$ is a linear transformation.

**Non-Example**: $T: \R^{2} \rightarrow \R^{2}, T((x,y)) = (x+1,y+1)$

Consider,
\begin{align*}
T((2,3)) &= (2+1,3+1)\\
         &= (3,4).
\end{align*}
Observe,
\begin{align*}
T(2(2,3)) &= T((4,6))\\
          &= (4+1,6+1)\\
          &= (5,7)
\end{align*}
and,
\begin{align*}
2T((2,3)) &= 2(3,4)\\
          &= (6,8).
\end{align*}

Since we have $T(2(2,3)) \neq 2T((2,3))$, $T$ doesn't preserve scalar multiplication, thus it's not a linear transformation.

**Example**: $T: \R^{2} \rightarrow \R^{2}, T((x,y)) = (x+y,x-y)$

Check 1) Scalar multiplication

Take $(x,y)\in\R^{2}, c\in\R$.
\begin{align*}
T(c(x,y)) &= T((cx,cy))\\
          &= (cx+cy,cx-cy)\\
          &= c(x+y,x-y)\\
          &= cT(x,y)
\end{align*}

Check 2) Vector addition

Take $(x,y), (a,b)\in\R^{2}$.
\begin{align*}
T((x,y)+(a,b)) &= T((x+a,y+b))\\
               &= ((x+a)+(y+b),(x+a)-(y+b))\\
               &= (x+a+y+b,x+a-y-b)\\
               &= (x+y,x-y)+(a+b,a-b)\\
               &= T((x,y)) + T((a,b))
\end{align*}

Therefore, $T$ is a linear transformation.

**Non-Example**: $T: \R^{2} \rightarrow \R, T(x,y) = \sqrt[3]{x^3+y^3}$

Check 1) Scalar multiplication

Take $(x,y)\in\R^{2}, c\in\R$.
\begin{align*}
T(c(x,y)) &= T((cx,cy)))\\
          &= \sqrt[3]{(cx)^3+(cy)^3}\\
          &= \sqrt[3]{c^3(x^3+y^3)}\\
          &= c^3\sqrt[3]{x^3+y^3}\\
          &= cT((x,y))
\end{align*}

Check 2) Vector addition

Consider, $(1,0), (0,1) \in\R^{2}$.
So,
\begin{align*}
T((1,0)) &= \sqrt[3]{1^3+0^3}\\
         &= 1
\end{align*}
and,
\begin{align*}
T((0,1)) &= \sqrt[3]{0^3+1^3}\\
         &= 1.
\end{align*}
Therefore,
\begin{align*}
T((1,0)) + T((0,1)) &= 1 + 1\\
                    &= 2.
\end{align*}
Also,
\begin{align*}
T((1,0) + (0,1)) &= T((1,1))\\
                 &= \sqrt[3]{1^3+1^3}\\
                 &= \sqrt[3]{2}.
\end{align*}

Since we have $T((1,0)) + T((0,1)) \neq T((1,0) + (0,1))$, $T$ doesn't preserve vector addition, thus it's not a linear transformation.

### Linear Transformations as Matrices

Associated with any linear transformation $T, T:R^{n} \rightarrow R^{m}$, is a matrix called the matrix of $T$.

**Theorem**: If $A$ is a $m \times n$ matrix, then $T_{A}: R^{n} \rightarrow R^{m}$ defined by $T_{A}(\vector{v}) = A\vector{v}$ is a linear transformation.

**Proof**:

Check 1) Scalar multiplication

\begin{align*}
T_{A}(c\vector{v}) &= A(c\vector{v})\\
                   &= cA\vector{v}\\
                   &= cT_{A}(\vector{v})
\end{align*}

Check 2) Vector addition

\begin{align*}
T_{A}(\vector{u}+\vector{v}) &= A(\vector{u}+\vector{v})\\
                             &= A(\vector{u}) + A(\vector{v})\\
                             &= T_{A}(\vector{u}) + T_{A}(\vector{v})
\end{align*}$\blacksquare$

**Example**: Consider $A = \begin{bmatrix} 1 & 1\\ 1 & -1\end{bmatrix}$. So,

\begin{align*}
T_{A}: \R^{2} \rightarrow \R^{2}, T_{A}((x,y)) = A\begin{bmatrix} x\\ y\end{bmatrix}&=\begin{bmatrix} 1 & 1\\ 1 & -1\end{bmatrix}\begin{bmatrix}x\\ y\end{bmatrix}\\
                                                                                    &= \begin{bmatrix}x + y\\ x - y\end{bmatrix}\\
                                                                                    &= (x+y,x-y).
\end{align*}

**Example**: Consider $A = \begin{bmatrix} 2 & 1\\ 3 & 7\\ 4 & 5\end{bmatrix}$. So,

\begin{align*}
T_{A}: \R^{2} \rightarrow \R^{3}, T_{A}((x,y)) = A\begin{bmatrix}x\\ y\end{bmatrix}&=\begin{bmatrix} 2 & 1\\ 3 & 7\\ 4 & 5\end{bmatrix}\begin{bmatrix}x\\ y\end{bmatrix}\\
                                                                                   &= \begin{bmatrix}2x + y\\ 3x + 7y\\ 4x + 5y\end{bmatrix}\\
                                                                                   &= (2x+y,3x+7y,4x+5y).
\end{align*}

**Definition**: Let $T: \R^{n} \rightarrow \R^{m}$ be a linear transformation. The matrix associated with $T$ is,

\begin{equation*}
A = \begin{bmatrix}(T\vector{e}_{1}) & (T\vector{e}_{2}) & (T\vector{e}_{3}) & ... & (T\vector{e}_{n})\end{bmatrix}
\end{equation*}
where $\vector{e}_{1}, \vector{e}_{2}, \vector{e}_{3}, ..., \vector{e}_{n}$ are the standard basis vectors of $\R^{n}$.

**Example**: $T((x,y,z)) = (2x-4y+3z,7x-2z)$

\begin{align*}
A &= \begin{bmatrix}T(1,0,0) & T(0,1,0) & T(0,0,1)\end{bmatrix}\\
  &= \begin{bmatrix}2 & -4 & 3\\ 7 & 0 & -2\end{bmatrix}\\
\end{align*}

$T_{A}: \R^{3} \rightarrow \R^{2}$

\begin{align*}
T_{A}((x,y,z)) &= \begin{bmatrix}2 & -4 & 3\\ 7 & 0 & -2\end{bmatrix}\begin{bmatrix}x\\ y\\ z\end{bmatrix}\\
               &= \begin{bmatrix}2x-4y+3z\\ 7x-2z\end{bmatrix}\\
               &= (2x-4y+3z,7x-2z)
\end{align*}

Notice this is equal to $T$, therefore $T_{A} = T$.

### The Image and Kernel of a Linear Transformation

Observe $T: V \rightarrow W$ where $V$ is the domain and $W$ is the codomain.

**Definition**: The **image** of $T$ (also known as the range of $T$) is

\begin{equation*}
\text{img}(T) = \{T\vector{v}:\vector{v}\in V\}.
\end{equation*}

**Definition**: The **kernel** of $T$ (also known as the nullspace of $T$) is

\begin{equation*}
\text{ker}(T) = \{T\vector{v}=\vector{0}:\vector{v}\in V\}.
\end{equation*}

**Theorem**: A linear transformation must take the zero vector of $V$ to the zero vector of $W$.

**Proof**:

\begin{align*}
T(\vector{0}_{V}) &= T(0\times\vector{0}_{V})\\
                  &= 0T(\vector{0}_{V})\\
                  &= \vector{0}_{W}
\end{align*}$\blacksquare$

**Non-Example**: $T((x,y)) = x^{2} + y^{2} + 1$

This is not a linear transformation because $T((0,0)) = 0 + 0 + 1 = 1$ which is not the zero vector.

**Non-Example**: $T: M_{2}(\R) \rightarrow M_{2}(\R), T\left(\begin{bmatrix}a & b\\ c & d\end{bmatrix}\right) = \begin{bmatrix}d & c\\ b & a+1\end{bmatrix}$

This is not a linear transformation because

\begin{equation*}
T\left(\begin{bmatrix}0 & 0\\ 0 & 0\end{bmatrix}\right) = \begin{bmatrix}0 & 0\\ 0 & 1\end{bmatrix}.
\end{equation*}

Clearly, this is not the zero vector.

Observe that

\begin{align*}
\text{ker}(T) = \{T\vector{v}=\vector{0}:\vector{v}\in V\}\subseteq{V}\\
\text{img}(T) = \{T\vector{v}:\vector{v}\in V\}\subseteq{W}.
\end{align*}

**Theorem**: $\text{ker}(T)$ is a subspace of $V$.

**Proof**: We want to show three things:

- $\vector{0}\in\text{ker}(T)$
- $\text{ker}(T)$ is closed under scalar multiplication
- $\text{ker}(T)$ is closed under vector addition.

Check 1) We proved $T(\vector{0}_{V}) = \vector{0}_{W}$, so $\vector{0}\in\text{ker}(T)$.

Check 2) Take $\vector{u}\in\text{ker}(T), c\in\R$. So, $T(\vector{u})=\vector{0}$. Then,

\begin{align*}
T(c\vector{u}) &= cT(\vector{u})\\
               &= c\vector{0}\\
               &= \vector{0}.
\end{align*}

Thus, $c\vector{u}\in\text{ker}(T)$.

Check 3) Take $u,v\in\text{ker}(T)$. So, $T(\vector{u})=\vector{0}$ and $T(\vector{v})=\vector{0}$. Then,

\begin{align*}
T(\vector{u}+\vector{v}) &= T(\vector{u}) + T(\vector{v})\\
                         &= \vector{0} + \vector{0}\\
                         &= \vector{0}.
\end{align*}

Thus, $\vector{u} + \vector{v} \in \text{ker}(T)$.

$\blacksquare$

**Example**: $T: P_{2}(\R) \rightarrow \R^{2}, T(a+bx+cx^{2}) = (a-b,a+c)$

Note: $P_{2}(\R) = \{p(x) \text{is a polynomial of degree at most } 2\}$

\begin{align*}
\text{ker}(T) &= \{p(x)\in P_{2}(\R)| T(p(x)) = (0,0)\}\\
              &= \{a + ax - ax^{2}| a\in\R\}\\
              &= \text{span}\{1+x-x^{2}\}
\end{align*}

**Example**: $D: C'(I) \rightarrow C^{0}(I), D(f(x)) = f'(x)$

\begin{align*}
\text{ker}(D) &= \{f(x)\in C^{1}(I) | D(f(x))=0\}\\
              &= \{f(x)\in C^{1}(I) | f'(x)=0\}\\
              &= \{f(x)\in C^{1}(I) | f(x) = a\text{ for some } a\in\R\}
\end{align*}

**Example**: $T:C^{2}(I) \rightarrow C^{0}(I), T(f(x)) = f''(x)-f(x)$

\begin{align*}
\text{ker}(T) &= \{f\in C^{2}(I)|T(f)=0\}\\
              &= \{f\in C^{2}(I)|f''(x)-f(x)=0\}\\
              &= \{f\in C^{2}(I)|f''(x)=f(x)\}\\
              &= \{C_{1}e^{x}+C_{2}e^{-x}| C_{1}, C{2}\in\R\}
\end{align*}

## Second-Order Linear Ordinary Differential Equations

**Definition**: A **second-order linear ODE** is an equation of the form
\begin{equation*}
a(x)y'' + b(x)y' + c(x)y = f(x)
\end{equation*}
where $a(x),b(x),c(x)$ are known as coefficient functions and $f(x)$ is known as the forcing function. This is **homogeneous** if $f(x)=0$, and has constant coefficients if $a(x),b(x),c(x)$ are all constant.

**Example**: $y'' + 0y' - y = 0$

So, $y'' = y$ with solutions $e^{x}$ and $e^{-x}$.

**Example**: $y'' + 0y' - 4y = 0$

So, $y'' = 4y$ with solutions $e^{2x}$ and $e^{-2x}$.

**Example**: $y'' + 3y' + 2y = 0$

Guess $y = e^{rx}$.
\begin{align*}
(e^{rx})'' + 3(e^{rx})' + 2(e^{rx}) &= 0\\
r^{2}e^{rx} + 3re^{rx} + 2e^{rx} &= 0\\
(r^{2}+3r+2)e^{rx} &= 0\\
r^{2} + 3r + 2 &= 0\\
(r+2)(r+1) &= 0\\
\end{align*}
So,
\begin{equation*}
r = -2, r = -1
\end{equation*}
which means
\begin{equation*}
y = e^{-2x}, y = e^{-x}.
\end{equation*}
Note that any linear combination of the two is also a solution. Thus, our general solution is
\begin{equation*}
y = C_{1}e^{-2x} + C_{2}e^{-x}.
\end{equation*}

**Example**: $y'' - y' - 6y = 0$

Guess $y = e^{rx}$.
\begin{align*}
(e^{rx})'' - (e^{rx})' - 6(e^{rx}) &= 0\\
r^{2}e^{rx} - re^{rx} - 6e^{rx} &= 0\\
(r^{2}-r-6)e^{rx} &= 0\\
r^{2} - r - 6 &= 0\\
(r+2)(r-3) &= 0\\
\end{align*}
So,
\begin{equation*}
r = -2, r = 3
\end{equation*}
which means
\begin{equation*}
y = e^{-2x}, y = e^{3x}.
\end{equation*}
Thus, our general solution is
\begin{equation*}
y = C_{1}e^{-2x} + C_{2}e^{3x}.
\end{equation*}

**Definition**: The **characteristic polynomial** of the second-order differential equation
\begin{equation*}
ay'' + by' + cy = 0
\end{equation*}
is the polynomial
\begin{equation*}
p(r) = ar^{2} + br + c.
\end{equation*}

### Applications

**Mass-spring system**

The equilibrium length of a mass-spring system is denoted as $l$. The distance of the mass to the system's equilibrium is denoted as $y$. The force of gravity is denoted as $F_{g}$ while the force of the spring is denoted as $F_{s}$. Respectively, $F_{g} = mg$ and $F_{s} = -k(y-l)$. So, the total force exerted on a mass in a mass-spring system is represented by the equation,
\begin{equation*}
F_{t} = -k(y-l).
\end{equation*}

However, notice that all forces are of the form $F=ma$, so
\begin{equation*}
-k(y-l) = my''.
\end{equation*}

Moving the left-hand side to the right means we have the differential equation
\begin{equation*}
my'' + k(y-l) = 0.
\end{equation*}

Let $\tilde{y} = y-l$ (relative length). Now the differential equation is
\begin{equation*}
m\tilde{y}'' + k\tilde{y} = 0.
\end{equation*}

**Pendulum**

From the second-order differential equation
\begin{equation*}
\frac{\mathrm{d}^{2}\theta}{\mathrm{d}t^{2}} + \frac{g}{l}\sin(\theta) = 0,
\end{equation*}
we can use the small-angle approximation where $\sin(\theta) \approx \theta$ to linearize our original differential equation as
\begin{equation*}
\frac{\mathrm{d}^{2}\theta}{\mathrm{d}t^{2}} + \frac{g}{l}\theta = 0.
\end{equation*}

**LCR-circuit**

\begin{align*}
LQ'' + RQ' + \frac{1}{C}Q &= 0\\
m\tilde{y}'' + \gamma\tilde{y}' + k\tilde{y}' &= 0
\end{align*}

## Linear Ordinary Differential Equations

**Definition**: An $n^{th}$ **order linear ODE** is an equation of the form

\begin{equation*}
a_{n}(x)y^{(n)} + a_{n-1}(x)y^{(n-1)} + a_{n-2}(x)y^{(n-2)} + ... + a_{1}(x)y' + a_{0}(x)y = f(x)
\end{equation*}

where $a_{n}(x), a_{n-1}(x), a_{n-2}(x), ..., a_{1}(x), a_{0}(x)$ are known as coefficient functions and $f(x)$ is known as the forcing function.

There's an important special case when $f(x) = 0$, we call this equation **homogeneous**, for example,

\begin{equation*}
my'' + \gamma y' + ky = 0.
\end{equation*}

### Initial Conditions

An initial condition for an $n^{th}$ order ODE is of the form

\begin{equation*}
y(x_{0}) = y_{0}, y'(x_{0}) = y_{1}, y''(x_{0}) = y_{2}, ..., y^{n-1}(x_{0}) = y_{n-1}.
\end{equation*}

An ODE with an initial condition is called an **initial value problem** (IVP).

### Existence and Uniqueness Theorem

Consider an $n^{th}$ order linear initial value problem

\begin{equation*}
a_{n}(x)y^{(n)} + a_{n-1}(x)y^{(n-1)} + a_{n-2}(x)y^{(n-2)} + ... + a_{1}(x)y' + a_{0}(x)y = f(x)
\end{equation*}

with $y(x_{0}) = y_{0}, y'(x_{0}) = y_{1}, y''(x_{0}) = y_{2}, ..., y^{(n-1)}(x_{0}) = y_{n-1}$.

Observe that if
- $I$ is an interval containing $x_{0}$
- $a_{0}(x), ..., a_{n}(x), f(x)$ are continuous on $I$
- $a_{n}(x) \neq 0$ on $I$
then the initial value problem has a unique solution on all of $I$.

**Example**: $y'' + 5y' + 4y = 0, y(0) = 1, y'(0) = 3$

Guess $y = e^{rt}$.
\begin{align*}
(e^{rt})'' + 5(e^{rt})' + 4(e^{rt}) &= 0\\
r^{2}e^{rt} + 5re^{rt} + 4e^{rt} &= 0\\
e^{rt}(r^{2}+5r+4) &= 0\\
r^{2} + 5r + 4 &= 0\\
(r+4)(r+1) &= 0
\end{align*}
So,
\begin{equation*}
r = -4, r = -1
\end{equation*}
which means
\begin{equation*}
y = e^{-4t}, y = e^{-t}.
\end{equation*}
Therefore, our general solution is
\begin{equation*}
y = C_{1}e^{-4t} + C_{2}e^{-t}.
\end{equation*}
Using $y(0) = 1$ we have,
\begin{align*}
1 &= C_{1}e^{0} + C_{2}e^{0}\\
1 &= C_{1} + C_{2}.
\end{align*}
Taking the derivative of $y$ we have,
\begin{equation*}
y' = -4C_{1}e^{-4t} - C_{2}e^{-t}.
\end{equation*}
Using $y'(0) = 3$ we have,
\begin{align*}
3 &= -4C_{1}e^{0} - C_{2}e^{0}\\
3 &= -4C_{1} - C_{2}.
\end{align*}
Now, we have the equations
\begin{align*}
1 &= C_{1} + C_{2}\\
3 &= -4C_{1} - C_{2}
\end{align*}
so
\begin{align*}
4 &= -3C_{1}\\
C_{1} &= -\frac{4}{3}\\
C_{2} &= 1 - \left(-\frac{4}{3}\right)\\
C_{2} &= \frac{7}{3}.
\end{align*}
Therefore, our solution to the initial value problem is
\begin{equation*}
y = -\frac{4}{3}e^{-4t} + \frac{7}{3}e^{-t}.
\end{equation*}

**Example**: $t^{2}y'' + 3ty' + y = 0, y(1) = 7, y'(1) = 2$

Guess $y = t^{r}$.
\begin{align*}
t^{2}(t^{r})'' + 3t(t^{r})' + (t^{r}) &= 0\\
t^{2}(r(r-1)t^{r-2}) + 3t(rt^{r-1}) + t^{r} &= 0\\
r(r-1)t^{r} + 3rt^{r} + t^{r} &= 0\\
(r^{2}+2r+1)t^{r} &= 0\\
r^{2} + 2r + 1 &= 0\\
(r+1)(r+1) &= 0
\end{align*}
So,
\begin{equation*}
r = -1, r = -1
\end{equation*}
which means
\begin{equation*}
y = t^{-1}, y = t^{-1}.
\end{equation*}
However, our true second solution is
\begin{equation*}
y = t^{-1}\ln{t}.
\end{equation*}
Therefore, our general solution is
\begin{equation*}
y = C_{1}t^{-1} + C_{2}t^{-1}\ln{t}.
\end{equation*}
Using $y(1) = 7$ we have,
\begin{align*}
7 &= C_{1}1^{-1} + C_{2}1^{-1}\ln{(1)}\\
C_{1} &= 7.
\end{align*}
Taking the derivative of $y$ we have,
\begin{align*}
y' &= -C_{1}t^{-2} - C_{2}t^{-2}\ln{t} + C_{2}t^{-2}\\
   &= -C_{1}t^{-2} + C_{2}(-t^{-2}\ln{t}+t^{-2}).
\end{align*}
Using $y'(1) = 2$ we have,
\begin{align*}
2 &= -C_{1}1^{-2} + C_{2}(-1^{-2}\ln{1}+1^{-2})\\
2 &= -C_{1} + C_{2}\\
C_{2} &= 9.
\end{align*}
Therefore, our solution to the initial value problem is
\begin{equation*}
y = 7t^{-1} + 9t^{-1}\ln{t}.
\end{equation*}