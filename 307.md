@def title = "307 | Notes"
@def tags = ["linear algebra", "ladr"]

# Linear Algebra

\tableofcontents

## Vector Spaces

\definition{vector space}{
A vector space is a set $V$ along with an addition and a scalar multiplication on $V$ such that the following properties hold.

**commutativity**\
$u+v=v+u$ for all $u,v\in V$.

**associativity**\
$(u+v)+w=u+(v+w)$ and $(ab)v=a(bv)$ for all $u,v,w\in V$ and $a,b\in\F$.

**additive identity**\
There exists an element $0\in V$ such that $v+0=v$ for all $v\in V$.

**additive inverse**\
For ever $v\in V$, there exists $w\in V$ such that $v+w=0$.

**multiplicative identity**\
$1v=v$ for all $v\in V$.

**distributive properties**\
$a(u+v)=au+av$ and $(a+b)v=av+bv$ for all $u,v\in V$ and $a,b\in\F$.
}

\definition{vector}{
Elements of a vector space are called vectors.
}

\theorem{unique additive identity}{
A vector space has a unique additive identity.
}

\theorem{unique additive inverse}{
Every element in a vector space has a unique additive inverse.
}

\notation{$-v,w-v$}{
Let $v,w\in V$, then

- $-v$ denotes the additive inverse of $v$

- $w-v$ is defined to be $w+(-v)$.
}

\theorem{the number $0$ times a vector}{
$0v=0$ for every $v\in V$.
}

\theorem{a number times the vector $0$}{
$a0=0$ for every $a\in\F$.
}

\theorem{the number $-1$ times a vector}{
$(-1)v=-v$ for every $v\in V$.
}

### Subspaces

\definition{subspace}{
A subset $U$ of $V$ is called a subspace of $V$ if $U$ is also a vector space with the same additive identity, addition, and scalar multiplication as on $V$.
}

\theorem{conditions for a subspace}{
A subset $U$ of $V$ is a subspace of $V$ if and only if $U$ satisfies the following three conditions.

**additive identity**\
$0\in U$.

**closed under addition**\
$u,w\in U$ implies $u+w\in U$.

**closed under scalar multiplication**\
$a\in\F$ and $u\in U$ implies $au\in U$
}

## Finite-Dimensional Vector Spaces

### Span and Linear Independence

\definition{linear combination}{
A linear combination of a list $\seq{v}{}{m}$ of vectors in $V$ is a vector of the form \equation{\combo{a}{m}{v}} where $\seq{a}{}{m}\in\F$.
}

\definition{span}{
The set of all linear combinations of a list of vectors $\seq{v}{}{m}$ in $V$ is called the span of $\seq{v}{}{m}$ denoted by $\rm{span}(\seq{v}{}{m})$. In other words, \equation{\rm{span}(\seq{v}{}{m})=\set{ \combo{a}{m}{v}:\seq{a}{}{m}\in\F }.}

The span of the empty list $()$ is defined to be $\set{0}$.
}

\theorem{span is the smallest containing subspace}{
The span of a list of vectors in $V$ is the smallest subspace of $V$ containing all vectors in the list.
}

\definition{spans}{
If $\rm{span}(\seq{v}{}{m})$ equals $V$, we say that the list $\seq{v}{}{m}$ spans $V$.
}

\definition{finite-dimensional vector space}{
A vector space is called finite-dimensional if some list of vectors in it spans the space.
}

\definition{infinite-dimensional vector space}{
A vector space is called infinite-dimensional if it is not finite-dimensional.
}

\definition{linearly independent}{
A list $\seq{v}{}{m}$ of vectors in $V$ is called linearly independent if the only choice of $\seq{a}{}{m}\in\F$ that makes \equation{\combo{a}{m}{v}=0} is $a_1=\dots=a_m=0$.

The empty list $()$ is also declared to be linearly independent.
}

\definition{linearly dependent}{
A list of vectors in $V$ is called linearly dependent if it is not linearly independent.

In other words, a list $\seq{v}{}{m}$ of vectors in $V$ is linearly dependent if there exists $\seq{a}{}{m}\in\F$, not all $0$, such that $\combo{a}{m}{v}=0$.
}

\theorem{length of linearly independent list $\leq$ length of spanning list}{
In a finite-dimensional vector space, the length of every linearly independent list of vectors is less than or equal to the length of every spanning list of vectors.
}

\theorem{finite-dimensional subspaces}{
Every subspace of a finite-dimensional vector space is finite-dimensional.
}

### Bases

\definition{basis}{
A basis of $V$ is a list of vectors in $V$ that is linearly independent and spans $V$.
}

\theorem{criterion to be a basis}{
A list $\seq{v}{}{m}$ of vectors in $V$ is a basis of $V$ if and only if every $v\in V$ can be written uniquely in the form \equation{v=\combo{a}{m}{v}} where $\seq{a}{}{m}\in\F$.
}

\theorem{every spanning list contains a basis}{
Every spanning list in a vector space can be reduced to a basis of the vector space.
}

\theorem{basis of finite-dimensional vector space}{
Every finite-dimensional vector space has a basis.
}

\theorem{every linearly independent list extends to a basis}{
Every linearly independent list of vectors in a finite-dimensional vector space can be extended to a basis of the vector space.
}

### Dimension

\theorem{basis length does not depend on basis}{
Any two bases of a finite-dimensional vector space have the same length.
}

\definition{dimension, $\dim V$}{
- The dimension of a finite-dimensional vector space is the length of any basis of the vector space.

- The dimension of a finite-dimensional vector space $V$ is denoted by $\dim V$.
}

\theorem{dimension of a subspace}{
If $V$ is finite-dimensional and $U$ is a subspace of $V$, then $\dim U\leq\dim V$.
}

\theorem{linearly independent list of the right length is a basis}{
Suppose $V$ is finite-dimensional, then every linearly independent list of vectors in $V$ of length $\dim V$ is a basis of $V$.
}

\theorem{subspace of full dimensional equals the whole space}{
Suppose that $V$ is finite-dimensional and $U$ is a subspace of $V$ such that $\dim U=\dim V$, then $U=V$.
}

\theorem{spanning list of the right length is a basis}{
Suppose $V$ is finite-dimensional, then every spanning list of vectors in $V$ of length $\dim V$ is a basis of $V$.
}

\theorem{dimension of a sum}{
If $V_1$ and $V_2$ are subspaces of a finite-dimensional vector space, then \equation{\dim(V_1+V_2)=\dim V_1+\dim V_2-\dim(V_1\cap V_2).}
}

## Linear Maps

\definition{linear map}{
A linear map from $V$ to $W$ is a function $\map{T}{V}{W}$ with the following properties.

**additivity**\
$T(u+v)=Tu+Tv$ for all $u,v\in V$.

**homogeneity**\
$T(\lambda v)=\lambda(Tv)$ for all $v\in V$ and all $\lambda\in\F$.
}

\notation{$\cal{L}(V,W),\cal{L}(V)$}{
- The set of linear maps from $V$ to $W$ is denoted by $\cal{L}(V,W)$.

- The set of linear maps from $V$ to $V$ is denoted by $\cal{L}(V)$. In other words, $\cal{L}(V)=\cal{L}(V,V)$.
}

\lemma{linear map lemma}{
Suppose $\seq{v}{}{m}$ is a basis of $V$ and $\seq{w}{}{m}\in W$, then there exists a unique linear map $\map{T}{V}{W}$ such that \equation{Tv_k=w_k} for each $k=1,\dots,m$.
}

\definition{addition and scalar multiplication on $\cal{L}(V,W)$}{
Suppose $S,T\in\cal{L}(V,W)$ and $\lambda\in\F$. The sum $S+T$ and the product $\lambda T$ are the linear maps from $V$ to $W$ defined by \equation{(S+T)(v)=Sv+Tv\qquad\text{and}\qquad(\lambda T)(v)=\lambda(Tv)} for all $v\in V$.
}

\theorem{$\cal{L}(V,W)$ is a vector space}{
With the operations of addition and scalar multiplication as defined above, $\cal{L}(V,W)$ is a vector space.
}

\definition{product of linear maps}{
If $T\in\cal{L}(U,V)$ and $S\in\cal{L}(V,W)$, then the product $ST\in\cal{L}(U,W)$ is defined by \equation{(ST)(u)=S(Tu)} for all $u\in U$.
}

\theorem{algebraic properties of products of linear maps}{
**associativity**\
$(T_1T_2)T_3=T_1(T_2T_3)$ whenever $T_1,T_2,$ and $T_3$ are linear maps such that the products make sense (meaning $T_3$ maps into the domain of $T_2$, and $T_2$ maps into the domain of $T_1$).

**identity**\
$TI=IT=T$ whenever $T\in\cal{L}(V,W)$; here the first $I$ is the identity operator on $V$, and the second $I$ is the identity operator on $W$.

**distributive properties**\
$(S_1+S_2)T=S_1T+S_2T$ and $S(T_1+T_2)=ST_1+ST_2$ whenever $T,T_1,T_2\in\cal{L}(U,V)$ and $S,S_1,S_2\in\cal{L}(V,W)$.
}

\theorem{linear maps take $0$ to $0$}{
Suppose $T$ is a linear map from $V$ to $W$, then $T(0)=0$.
}

### Kernels and Images

\definition{kernel, $\ker T$}{
For $T\in\cal{L}(V,W)$, the kernel of $T$, denoted by $\ker T$, is the subset of $V$ consisting of those vectors that $T$ maps to $0$, i.e., \equation{\ker T=\set{ v\in V: Tv=0}. }
}

\theorem{the kernel is a subspace}{
Suppose $T\in\cal{L}(V,W)$, then $\ker T$ is a subspace of $V$.
}

\definition{injective}{
A function $\map{T}{V}{W}$ is called injective if $Tu=Tv$ implies $u=v$.
}

\theorem{injectivity $\iff$ kernel equals $\set{0}$}{
Let $T\in\cal{L}(V,W)$, then T is injective if and only if $\ker T=\set{0}$.
}

\definition{image}{
For $T\in\cal{L}(V,W)$, the image of $T$ is the subset of W consisting of those vectors that are equal to $Tv$ for some $v\in V$,i.e., \equation{\rm{im}\ T=\set{ Tv:v\in V }.}
}

\theorem{the image is a subspace}{
If $T\in\cal{L}(V,W)$, then $\rm{im}\ T$ is a subspace of $W$.
}

\definition{surjective}{
A function $\map{T}{V}{W}$ is called surjective if its image equals $W$.
}

\theorem{fundamental theorem of linear maps}{
Suppose $V$ is finite-dimensional and $T\in\cal{L}(V,W)$, then $\rm{im}\ T$ is finite-dimensional and \equation{\dim V=\dim\ker T +\dim\rm{im}\ T.}
}

\theorem{linear map to a lower-dimensional space is not injective}{
Suppose $V$ and $W$ are finite-dimensional vector spaces such that $\dim V>\dim W$, then no linear map from $V$ to $W$ is injective. 
}

\theorem{linear map to a higher-dimensional space is not surjective}{
Suppose $V$ and $W$ are finite-dimensional vector spaces such that $\dim V<\dim W$, then no linear map from $V$ to $W$ is surjective.
}

### Matrices

\definition{matrix, $A_{j,k}$}{
Suppose $m$ and $n$ are non-negative integers. An $m\times n$ matrix $A$ is a rectangular array of elements of $\F$ with $m$ rows and $n$ columns,i.e., \equation{A=\matrix{A_{1,1} & \dots & A_{1,n}\\ \vdots & & \vdots\\ A_{m,1} & \dots & A_{m,n}}.}

The notation $A_{j,k}$ denotes the entry in row $j$, column $k$ of A.
}

\definition{matrix of a linear map, $\cal{M}(T)$}{
Suppose $T\in\cal{L}(V,W)$ and $\seq{v}{}{n}$ is a basis of $V$ and $\seq{w}{}{m}$ is a basis of $W$. The matrix of $T$ with respect to these bases is the $m\times n$ matrix $\cal{M}(T)$ whose entries $A_{j,k}$ are defined by \equation{Tv_k=A_{1,k}w_1+\dots+A_{m,k}w_m.}

If the bases $\seq{v}{}{n}$ and $\seq{w}{}{m}$ are not clear from the context, then the notation $\cal{M}(T,(\seq{v}{}{n}),(\seq{w}{}{m}))$ is used.
}

\definition{matrix addition}{
The sum of two matrices of the same size is the matrix obtained by adding corresponding entries in the matrices,i.e., \equation{\matrix{A_{1,1} & \dots & A_{1,n}\\ \vdots & & \vdots\\ A_{m,1} & \dots & A_{m,n}}+\matrix{B_{1,1} & \dots & B_{1,n}\\ \vdots & & \vdots\\ B_{m,1} & \dots & B_{m,n}}=\matrix{A_{1,1}+B_{1,1} & \dots & A_{1,n}+B_{1,n}\\ \vdots & & \vdots\\ A_{m,1}+B_{m,1} & \dots & A_{m,n}+B_{m,n}}.}
}

\theorem{matrix of the sum of linear maps}{
Suppose $S,T\in\cal{L}(V,W)$, then $\cal{M}(S+T)=\cal{M}(S)+\cal{M}(T)$.
}

\definition{scalar multiplication of a matrix}{
The product of a scalar and a matrix is the matrix obtained by multiplying each entry in the matrix by the scalar,i.e., \equation{\lambda\matrix{A_{1,1} & \dots & A_{1,n}\\ \vdots & & \vdots\\ A_{m,1} & \dots & A_{m,n}}=\matrix{\lambda A_{1,1} & \dots & \lambda A_{1,n}\\ \vdots & & \vdots\\ \lambda A_{m,1} & \dots & \lambda A_{m,n}}.}
}

\theorem{the matrix of a scalar times a linear map}{
Suppose $T\in\cal{L}(V,W)$ and $\lambda\in\F$, then $\cal{M}(\lambda T)=\lambda\cal{M}(T)$.
}

\notation{$\F^{m,n}$}{
For $m$ and $n$ positive integers, the set of all $m\times n$ matrices with entries in $\F$ is denoted by $\F^{m,n}$.
}

\theorem{$\dim\F^{m,n}=mn$}{
Suppose $m$ and $n$ are positive integers. With addition and scalar multiplication defined as above, $\F^{m,n}$ is a vector space of dimension $mn$.
}

\definition{matrix multiplication}{
Suppose $A$ is an $m\times n$ matrix and $B$ is an $n\times p$ matrix, then $AB$ is defined to be the $m\times p$ matrix whose entry in row $j$, column $k$, is given by the equation \equation{(AB)_{j,k}=\sum_{r=1}^nA_{j,r}B_{r,k}.}Thus, the entry in row $j$, column $k$, of $AB$ is computed by taking row $j$ of $A$ and column $k$ of $B$, multiplying together corresponding entries, and then summing.
}

\theorem{matrix of product of linear maps}{
If $T\in\cal{L}(U,W)$ and $S\in\cal{L}(V,W)$, then $\cal{M}(ST)=\cal{M}(S)\cal{M}(T)$.
}

\notation{$A_{j,.},A_{.,k}$}{
Suppose $A$ is an $m\times n$ matrix.

- If $1\leq j\leq m$, then $A_{j,.}$ denotes the $1\times n$ matrix consisting of row $j$ of $A$.

- If $1\leq k\leq n$, then $A_{.,k}$ denotes the $m\times 1$ matrix consisting of column $k$ of $A$.
}

\definition{column rank, row rank}{
Suppose $A$ is an $m\times n$ matrix with entries in $\F$.

- The column rank of $A$ is the dimension of the span of the columns of $A$ in $\F^{m,1}$.

- The row rank of $A$ is the dimension of the span of the rows of $A$ in $\F^{1,n}$.
}

\theorem{column rank equals row rank}{
Suppose $A\in\F^{m,n}$, then the column rank of $A$ equals the row rank of $A$.
}

\definition{rank}{
The rank of a matrix $A\in\F^{m,n}$ is the column rank of $A$.
}

### Invertibility and Isomorphisms

\definition{invertible, inverse}{
- A linear map $T\in\cal{L}(V,W)$ is called invertible if there exists a linear map $S\in\cal{L}(W,V)$ such that $ST$ equals the identity operator on $V$ and $TS$ equals the identity operator on $W$.

- A linear map $S\in\cal{L}(W,V)$ satisfying $ST=I$ and $TS=I$ is called an inverse of $T$ (note that the first $I$ is the identity operator on $V$ and the second $I$ is the identity operator on $W$).
}

\theorem{inverse is unique}{
An invertible linear map has a unique inverse.
}

\notation{$T^{-1}$}{
If $T$ is invertible, then its inverse is denoted by $T^{-1}$. In other words, if $T\in\cal{L}(V,W)$ is invertible, then $T^{-1}$ is the unique element of $\cal{L}(V,W)$ such that $T^{-1}T=I$ and $TT^{-1}=I$.
}

\theorem{invertibility $\iff$ injectivity and surjectivity}{
A linear map is invertible if and only if it is injective and surjective.
}

\theorem{injectivity is equivalent to surjectivity (if $\dim V =\dim W<\infty$)}{
Suppose that $V$ and $W$ are finite-dimensional vector spaces, $\dim V=\dim W$, and $T\in\cal{L}(V,W)$. Then, \equation{T\text{ is inveritible}\quad\iff\quad T\text{ is injective}\quad\iff\quad T\text{ is surjective.}}
}

\theorem{$ST=I\iff TS=I$ (on vector spaces of the same dimension)}{
Suppose $V$ and $W$ are finite-dimensional vector spaces of the same dimension, $S\in\cal{L}(W,V)$, and $T\in\cal{L}(V,W)$. Then, $ST=I$ if and only if $TS=I$.
}

\definition{isomorphism, isomorphic}{
- An isomorphism is an invertible linear map.

- Two vector spaces are called isomorphic if there is an isomorphism from one vector space onto the other one.
}

\theorem{dimension shows whether vector spaces are isomorphic}{
Two finite-dimensional vector spaces over $\F$ are isomorphic if and only if they have the same dimension.
}

\theorem{$\cal{L}(V,W)$ and $\F^{m,n}$ are isomorphic}{
Suppose $\seq{v}{}{n}$ is a basis of $V$ and $\seq{w}{}{m}$ is a basis of $W$, then $\cal{M}$ is an isomorphism between $\cal{L}(V,W)$ and $\F^{m,n}$.
}

\theorem{$\dim\cal{L}(V,W)=(\dim V)(\dim W)$}{
Suppose $V$ and $W$ are finite-dimensional, then $\cal{L}(V,W)$ is finite-dimensional and \equation{\dim\cal{L}(V,W)=(\dim V)(\dim W).}
}

\definition{matrix of a vector, $\cal{M}(v)$}{
Suppose $v\in V$ and $\seq{v}{}{m}$ is a basis of $V$. The matrix of $v$ with respect to this basis is the $m\times 1$ matrix \equation{\cal{M}(v)=\matrix{b_1\\ \vdots\\ b_m},} where $\seq{b}{}{m}$ are the scalars such that \equation{v=\combo{b}{m}{v}.}
}

\theorem{$\cal{M}(T)_{.,k}=\cal{M}(Tv_k)$}{
Suppose $T\in\cal{L}(V,W)$ and $\seq{v}{}{n}$ is a basis of $V$ and $\seq{w}{}{m}$ is a basis of $W$. Let $1\leq k\leq n$. Then the $k^{\text{th}}$ column of $\cal{M}(T)$, which is denoted by $\cal{M}(T)_{.,k}$, equals $\cal{M}(Tv_k)$.
}

\theorem{linear maps act like matrix multiplication}{
Suppose $T\in\cal{L}(V,W)$ and $v\in V$. Suppose $\seq{v}{}{n}$ is a basis of $V$ and $\seq{w}{}{m}$ is a basis of $W$. Then, \equation{\cal{M}(Tv)=\cal{M}(T)\cal{M}(v).}
}

\theorem{dimension of $\rm{im}\ T$ equals column rank of $\cal{M}(T)$}{
Suppose $V$ and $W$ are finite-dimensional and $T\in\cal{L}(V,W)$. Then, $\dim\rm{im}\ T$ equals the column rank of $\cal{M}(T)$.
}

\definition{identity matrix, $I$}{
Suppose $n$ is a positive integer. The $n\times n$ matrix \equation{\matrix{1 & & 0\\ & \ddots & \\ 0 & & 1}} with $1$'s on the diagonal (where row number equals column number) and $0$'s elsewhere is called the identity matrix and is denoted by $I$.
}

\theorem{change-of-basis formula}{
Suppose $T\in\cal{L}(V)$. Suppose $\seq{u}{}{n}$ and $\seq{v}{}{n}$ are bases of $V$. Let \equation{A=\cal{M}(T,(\seq{u}{}{n}))\quad\text{ and }\quad B=\cal{M}(T,(\seq{v}{}{n}))} and $C=\cal{M}(I,(\seq{u}{}{n}),(\seq{v}{}{n}))$. Then, \equation{A=C^{-1}BC.}
}

\theorem{matrix of inverse equals inverse of matrix}{
Suppose that $\seq{v}{}{n}$ is a basis of $V$ and $T\in\cal{L}(V)$ is invertible. Then, $\cal{M}(T^{-1})=(\cal{M}(T))^{-1}$, where both matrices are with respect to the basis $\seq{v}{}{n}$.
}

## Eigenvalues and Eigenvectors

### Invariant Subspaces

\definition{operator}{
A linear map from a vector space to itself is called an operator.
}

\definition{invariant subspace}{
Suppose $T\in\cal{L}(V)$. A subspace $U$ of $V$ is called invariant under $T$ if $Tu\in U$ for every $u\in U$.
}

\definition{eigenvalue}{
Suppose $T\in\cal{L}(V)$. A number $\lambda\in\F$ is called an eigenvalue of $T$ if there exists $v\in V$ such that $v\neq 0$ and $Tv=\lambda v$.
}

\theorem{equivalent conditions to be an eigenvalue}{
Suppose $V$ is finite-dimensional, $T\in\cal{L}(V)$, and $\lambda\in\F$. Then, the following are equivalent.

- $\lambda$ is an eigenvalue of $T$.

- $T-\lambda I$ is not injective.

- $T-\lambda I$ is not surjective.

- $T-\lambda I$ is not invertible.
}

\definition{eigenvector}{
Suppose $T\in\cal{L}(V)$ and $\lambda\in\F$ is an eigenvalue of $T$. A vector $v\in V$ is called an eigenvector of $T$ corresponding to $\lambda$ if $v\neq 0$ and $Tv=\lambda v$.
}

\theorem{linearly independent eigenvectors}{
Suppose $T\in\cal{L}(V)$. Then, every list of eigenvectors of $T$ corresponding to distinct eigenvalues of $T$ is linearly independent.
}

\theorem{operator cannot have more eigenvalues than dimension of vector space}{
Suppose $V$ is finite-dimensional. Then, each operator on $V$ has at most $\dim V$ distinct eigenvalues.
}

\notation{$T^m$}{
Suppose $T\in\cal{L}(V)$ and $m$ is a positive integer.

- $T^m\in\cal{L}(V)$ is defined by $T^m=T\cdots T$.

- $T^0$ is defined to be the identity operator $I$ on $V$.

- If $T$ is invertible with inverse $T^{-1}$, then $T^{-m}\in\cal{L}(V)$ is defined by \equation{T^{-m}=(T^{-1})^m.}
}

\notation{p(T)}{
Suppose $T\in\cal{L}(V)$ and $p\in\cal{P}(\F)$ is a polynomial given by \equation{p(z)=\poly{a}{m}{z}} for all $z\in\F$. Then, $p(T)$ is the operator on $V$ defined by \equation{p(T)=a_0I+a_1T+a_2T^2+\dots+a_mT^m.}
}

\definition{product of polynomials}{
If $p,q\in\cal{P}(\F)$, then $pq\in\cal{P}(\F)$ is the polynomial defined by \equation{(pq)(z)=p(z)q(z)} for all $z\in\F$.
}

\theorem{multiplicative properties}{
Suppose $p,q\in\cal{P}(\F)$ and $T\in\cal{L}(V)$. Then,

- $(pq)(T)=p(T)q(T)$.

- $p(T)q(T)=q(T)p(T)$.
}

\theorem{kernel and image of $p(T)$ are invariant under $T$}{
Suppose $T\in\cal{L}(V)$ and $p\in\cal{P}(\F)$. Then, $\ker p(T)$ and $\rm{im}\ p(T)$ are invariant under $T$.
}

### The Minimal Polynomial

\theorem{existence of eigenvalues}{
Every operator on a finite-dimensional non-zero complex vector space has an eigenvalue.
}

\definition{monic polynomial}{
A monic polynomial is a polynomial whose highest-degree coefficient equals $1$.
}

\theorem{existence, uniqueness, and degree of minimal polynomial}{
Suppose $V$ is finite-dimensional and $T\in\cal{L}(V)$. Then, there is a unique monic polynomial $p\in\cal{P}(\F)$ of smallest degree such that $p(T)=0$. Furthermore, $\rm{deg}\ p\leq\dim V$.
}

\definition{minimal polynomial}{
Suppose $V$ is finite-dimensional and $T\in\cal{L}(V)$. Then, the minimal polynomial of $T$ is the unique monic polynomial $p\in\cal{P}(\F)$ of smallest degree such that $p(T)=0$.
}

\theorem{eigenvalues are the zeros of the minimal polynomial}{
Suppose $V$ is finite-dimensional and $T\in\cal{L}(V)$.

- The zeros of the minimal polynomial of $T$ are the eigenvalues of $T$.

- If $V$ is a complex vector space, then the minimal polynomial of $T$ has the form \equation{(z-\lambda_1)\cdots(z-\lambda_m),} where $\seq{\lambda}{}{m}$ is a list of all eigenvalues of $T$, possibly with repetitions.
}

\theorem{$q(T)=0\iff q$ is a polynomial multiple of the minimal polynomial}{
Suppose $V$ is finite-dimensional, $T\in\cal{L}(V)$, and $q\in\cal{P}(\F)$. Then, $q(T)=0$ if and only if $q$ is a polynomial multiple of the minimal polynomial of $T$.
}

\theorem{$T$ not invertible $\iff$ constant term of minimal polynomial of $T$ is $0$}{
Suppose $V$ is finite-dimensional and $T\in\cal{L}(V)$. Then, $T$ is not invertible if and only if the constant term of the minimal polynomial of $T$ is $0$.
}

\theorem{operators on odd-dimensional vector spaces have eigenvalues}{
Every operator on an odd-dimensional vector space has an eigenvalue.
}

### Upper-Triangular Matrices

\definition{diagonal of a matrix}{
The diagonal of a square matrix consists of the entries on the line from the upper left corner to the bottom right corner.
}

\definition{upper-triangular matrix}{
A square matrix is called upper-triangular if all entries below the diagonal are $0$.
}

\theorem{conditions for upper-triangular matrix}{
Suppose $T\in\cal{L}(V)$ and $\seq{v}{}{m}$ is a basis of $V$. Then, the following are equivalent.

- The matrix of $T$ with respect to $\seq{v}{}{m}$ is upper-triangular.

- $\rm{span}(\seq{v}{}{k})$ is invariant under $T$ for each $k=1,\dots,m$.

- $Tv_k\in\rm{span}(\seq{v}{}{k})$ for each $k=1,\dots,m$.
}

\theorem{equation satisfied by operator with upper-triangular matrix}{
Suppose $T\in\cal{L}(V)$ and $V$ has a basis with respect to which $T$ has an upper-triangular matrix with diagonal entries $\seq{\lambda}{}{m}$. Then, \equation{(T-\lambda_1 I)\cdots(T-\lambda_m I)=0.}
}

\theorem{determination of eigenvalues from upper-triangular matrix}{
Suppose $T\in\cal{L}(V)$ has an upper-triangular matrix with respect to some basis of $V$. Then, the eigenvalues of $T$ are precisely the entries on the diagonal of that upper-triangular matrix.
}

\theorem{necessary and sufficient condition to have an upper-triangular matrix}{
Suppose $V$ is finite-dimensional and $T\in\cal{L}(V)$. Then, $T$ has an upper-triangular matrix with respect to some basis of $V$ if and only if the minimal polynomial of $T$ equals $(z-\lambda_1)\cdots(z-\lambda_m)$ for some $\seq{\lambda}{}{m}\in\F$.
}

\theorem{if $\F=\C$, then every operator on $V$ has an upper-triangular matrix}{
Suppose $V$ is a finite-dimensional complex vector space and $T\in\cal{L}(V)$. Then, $T$ has an upper-triangular matrix with respect to some basis of $V$.
}

### Diagonalizable Operators

\definition{diagonal matrix}{
A diagonal matrix is a square matrix that is $0$ everywhere except possibly on the diagonal.
}

\definition{diagonalizable}{
An operator on $V$ is called diagonalizable if the operator has a diagonal matrix with respect to some basis of $V$.
}

\definition{eigenspace, $E(\lambda,T)$}{
Suppose $T\in\cal{L}(V)$ and $\lambda\in\F$. The eigenspace of $T$ corresponding to $\lambda$ is the subspace $E(\lambda,T)$ of $V$ defined by \equation{E(\lambda,T)=\ker(T-\lambda I)=\set{v\in V:Tv=\lambda v}} Hence $E(\lambda,T)$ is the set of all eigenvectors of $T$ corresponding to $\lambda$, along with the $0$ vector.
}

\theorem{sum of eigenspaces is a direct sum}{
Suppose $T\in\cal{L}(V)$ and $\seq{\lambda}{}{m}$ are distinct eigenvalues of $T$. Then, \equation{E(\lambda_1,T)+\cdots+E(\lambda_m,T)} is a direct sum. Furthermore, if $V$ is finite-dimensional, then \equation{\dim E(\lambda_1,T)+\cdots+\dim E(\lambda_m,T)\leq\dim V.}

}

\theorem{conditions equivalent to diagonalizability}{
Suppose $V$ is finite-dimensional and $T\in\cal{L}(V)$. Let $\seq{\lambda}{}{m}$ denote the distinct eigenvalues of $T$. Then, the following are equivalent.

- $T$ is diagonalizable.

- $V$ has a basis consisting of eigenvectors of $T$.

- $V=E(\lambda_1,T)\oplus\cdots\oplus E(\lambda_m,T)$.

- $\dim V=\dim E(\lambda_1,T)+\cdots+\dim E(\lambda_m,T)$.
}

\theorem{enough eigenvalues implies diagonalizability}{
Suppose $V$ is finite-dimensional and $T\in\cal{L}(V)$ has $\dim V$ distinct eigenvalues. Then, $T$ is diagonalizable.
}

\theorem{necessary and sufficient condition for diagonalizability}{
Suppose $V$ is finite-dimensional and $T\in\cal{L}(V)$. Then, $T$ is diagonalizable if and only if the minimal polynomial of $T$ equals $(z-\lambda_1)\cdots(z-\lambda_m)$ for some list of distinct numbers $\seq{\lambda}{}{m}\in\F$.
}

## Inner Product Spaces

### Inner Products and Norms

### Orthonormal Bases

### Orthogonal Complements and Minimization Problems
